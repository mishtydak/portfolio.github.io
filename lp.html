<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="(link unavailable)">
    <link rel="stylesheet" href="style.css">
    <title>Portfolio Website</title>
    <style>
        h2 {
            font-size: 28px;
            margin-bottom: 10px;
        }
        ul, li {
            font-size: 18px;
            line-height: 1.8;
        }
        p {
            font-size: 18px;
        }
        a {
            text-decoration: none;
            color: #0066cc;
        }
        code {
            font-size: 20px;
            background-color: #f0f0f0;
            padding: 10px;
            border: 1px solid #ddd;
        }
        table {
            border-collapse: collapse;
            width: 100%;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 10px;
            text-align: left;
        }
        th {
            background-color: #f0f0f0;
        }
    </style>
</head>
<body>
    <header>
        <a href="#" class="logo">Data Analysis and Algorithms</a>
    </header>
    <section class="home">
        <div class="home-img">
            <img src="(link unavailable)" alt="Data Analysis Image" style="width: 100%; height: 500px; object-fit: cover;">
        </div>
    </section>
    <section class="content">
        <h2>1. Problems in Nature: Iteration, Recursion, Backtracking</h2>
        <p>Problems in nature can be solved using iteration, recursion, or backtracking. Iteration involves repeating steps, like counting animal populations or watching seasonal changes. Recursion solves problems by breaking them into smaller parts, such as the patterns of tree branches or fractals in snowflakes. Backtracking explores all possible options and returns to try a different path if needed, like ants searching for food or animals solving mazes.</p>
        <h3>Iteration Example: Factorial Calculation</h3>
        <pre><code>long long factorial (int n) {
    long long result = 1;
    for (int i = 1; i <= n; ++i) {
        result *= i;
    }
    return result;
}</code></pre>
        <h3>Recursion Example: Fibonacci Series</h3>
        <pre><code>int fibonacci(int n) {
    if (n <= 1) {
        return n;
    }
    return fibonacci(n - 1) + fibonacci(n - 2);
}</code></pre>
        <h3>Recursion Example: Tower of Hanoi</h3>
        <pre><code>#include <stdio.h>
void towers(int n, char from, char to, char aux) {
    if (n == 1) {
        printf("Move disk 1 from %c to %c\n", from, to);
        return;
    }
    towers(n - 1, from, aux, to);
    printf("Move disk %d from %c to %c\n", n, from, to);
    towers(n - 1, aux, to, from);
}
int main() {
    int n;
    printf("Enter the number of Disks to be moved\n");
    scanf("%d", &n);
    towers(n, 'A', 'C', 'B');
    return 0;
}</code></pre>
        <h3>Backtracking Example: N-Queens Problem</h3>
        <pre><code>#include <iostream>
using namespace std;
bool isSafe(int board[][10], int row, int col, int N) {
    for (int i = 0; i < row; i++) {
        if (board[i][col] == 1 || (col - (row - i) >= 0 && board[i][col - (row - i)] == 1) || (col + (row - i) < N && board[i][col + (row - i)] == 1)) {
            return false;
        }
    }
    return true;
}
bool solveNQueens(int board[][10], int row, int N) {
    if (row == N) {
        for (int i = 0; i < N; i++) {
            
            for (int j = 0; j < N; j++) {
                cout << (board[i][j] ? "Q " : ". ");
            }
            cout << endl;
        }
        cout << endl;
        return true;
    }
    bool res = false;
    for (int col = 0; col < N; col++) {
        if (isSafe(board, row, col, N)) {
            board[row][col] = 1;
            res = solveNQueens(board, row + 1, N) || res;
            board[row][col] = 0;
        }
    }
    return res;
}
void solveNQueens(int N) {
    int board[10][10] = {0};
    if (!solveNQueens(board, 0, N)) {
        cout << "Solution does not exist!" << endl;
    }
}
int main() {
    int N;
    cout << "Enter the value of N: ";
    cin >> N;
    solveNQueens(N);
    return 0;
}</code></pre>
        <h3>2. What is space and time efficiency? Why are they important?</h3>
        <table>
            <thead>
                <tr>
                    <th>Order</th>
                    <th>Description</th>
                    <th>Example</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>O(1)</td>
                    <td>Constant time</td>
                    <td>Accessing an element in an array</td>
                </tr>
                <tr>
                    <td>O(log n)</td>
                    <td>Logarithmic time</td>
                    <td>Binary search</td>
                </tr>
                <tr>
                    <td>O(n)</td>
                    <td>Linear time</td>
                    <td>Linear search</td>
                </tr>
                <tr>
                    <td>O(n log n)</td>
                    <td>Log-linear time</td>
                    <td>Sorting algorithms like MergeSort</td>
                </tr>
                <tr>
                    <td>O(n²)</td>
                    <td>Quadratic time</td>
                    <td>Bubble Sort</td>
                </tr>
                <tr>
                    <td>O(2ⁿ)</td>
                    <td>Exponential time</td>
                    <td>Solving the Tower of Hanoi</td>
                </tr>
            </tbody>
        </table>
        <p>Space efficiency is about how much memory an algorithm uses, and time efficiency is about how fast it runs. These are important because they help us choose algorithms that can handle large tasks without using too much memory or taking too long. For example, algorithms that take constant time (O(1)) or logarithmic time (O(log n)) are very fast. However, those with quadratic (O(n²)) or exponential time (O(2ⁿ)) can be too slow for large problems. Choosing efficient algorithms is crucial in real-world tasks like processing data or running programs smoothly.</p>
        <h3>3. Takeaways from Chapter 2 Design Principles</h3>
        <p>Key design principles include divide and conquer, greedy methods, dynamic programming, and abstraction. Divide and conquer breaks problems into smaller pieces, solves them, and combines the results, like in merge sort. Greedy methods focus on making the best choice at every step, such as finding the shortest route in a map. Dynamic programming stores solutions to smaller problems to avoid repeating work, like calculating Fibonacci numbers. Abstraction helps simplify problems by focusing only on what matters and ignoring extra details, like in object-oriented programming.</p>
        <h4>Divide and Conquer</h4>
        <pre><code>ALGORITHM MergeSort(A[0..n-1])
if n > 1
    copy A[0...n/2 - 1] to B[0...n/2 - 1]
    copy A[n/2 ... n - 1] to C[0...n/2 - 1]
    MergeSort(B)
    MergeSort(C)
    Merge(B, C, A)
ALGORITHM Merge(B, C, A)
i <- 0
j <- 0
k <- 0
while i < p and j < q
    if B[i] <= C[j]
        A[k] <- B[i]
        i <- i + 1
    else
        A[k] <- C[j]
        j <- j + 1
    k <- k + 1
if i == p
    copy C[j...q-1] to A[k...p+q-1]
else
    copy B[i...p-1] to A[k...p+q-1]</code></pre>

        <p>Break a problem into smaller sub-problems, solve them, and combine results (e.g., MergeSort, QuickSort).</p>
        <h3>4. The Hierarchical Data and Tree Data Structures</h3>
        <img src="(link unavailable)" alt="Tree Data Structure" style="width: 100%; height: 500px; object-fit: cover;">
        <p>Tree structures are useful for organizing and solving problems efficiently. Binary search trees (BSTs) are good for fast searching and sorting. Self-balancing trees like AVL trees or red-black trees ensure operations remain quick even as the tree grows. Tries are special trees for working with words, such as in auto-complete suggestions. Heaps are trees used to manage priorities, such as finding the largest or smallest item quickly. These structures are essential for handling complex tasks in an organized way.</p>
        <ul>
            <li><strong>Tree</strong>: A hierarchical structure with a root node and child nodes.</li>
            <li><strong>Binary Search Tree (BST)</strong>: A tree where each node has at most two children, with the left child smaller and the right child larger than the parent.</li>
            <li><strong>AVL Tree</strong>: A self-balancing binary search tree that maintains balance during insertion and deletion.</li>
            <li><strong>Heap</strong>: A complete binary tree where parent nodes are compared to child nodes.</li>
            <li><strong>Trie:</strong> Specialized for prefix-based search.</li>
            <li><strong>Red-Black Tree:</strong> A balanced BST ensuring logarithmic time operations.</li>
        </ul>
    </section>
    <section id="array-query">
        <h2>The Need for Array Query Algorithms</h2>
        <img src="(link unavailable)" alt="Segment Tree Example" style="width: 100%; height: 500px; object-fit: cover;">
        <p>Array query algorithms help retrieve and update data quickly. For example, prefix sums can calculate the total of a range in an array very fast. Segment trees and Fenwick trees are advanced tools that allow for both updates and queries efficiently. These algorithms are used in applications like tracking scores, stock prices, or rainfall over time.</p>
        <ul>
            <li><strong>Segment Tree:</strong> Handles range queries efficiently.</li>
            <li><strong>Fenwick Tree:</strong> Optimized for cumulative frequency queries.</li>
            <li><strong>Sparse Table:</strong> Precomputes answers for range queries.</li>
        </ul>
    </section>
    <section id="trees-vs-graphs">
        <h2>Differentiation Between Trees and Graphs</h2>
        <img src="(link unavailable)" alt="Trees vs Graphs" style="width: 100%; height: 500px; object-fit: cover;">
        <p>Trees and graphs represent data differently and are used for distinct purposes. Trees are hierarchical with a single root and no cycles, like family trees or file systems. Graphs, on the other hand, are more general, allowing multiple connections and cycles, such as in social networks or transportation systems. Trees have specific traversals like preorder, inorder, and postorder for structured data exploration, while graphs use BFS and DFS for navigating interconnected data.</p>
    </section>
    <section id="sorting-searching">
        <h2>Sorting and Searching Algorithms in Real-World Contexts</h2>
        <img src="(link unavailable)" alt="Sorting Algorithms" style="width: 100%; height: 500px; object-fit: cover;">
        <p>Sorting and searching algorithms are essential for managing and retrieving data efficiently. Sorting algorithms, such as merge sort and quick sort, organize data in a specified order, which is vital for applications like e-commerce and data analysis. Merge sort uses a divide-and-conquer strategy, while quick sort selects a pivot to partition and sort the data. Searching algorithms, such as binary search, allow for quick retrieval of items from sorted lists by halving the search space repeatedly. This makes them ideal for applications like finding records in databases or locating files.</p>
    </section>
    <section id="graph-algorithms">
        <h2>Importance of Graph Algorithms</h2>
        <img src="(link unavailable)" alt="Graph Algorithms" style="width: 100%; height: 500px; object-fit: cover;">
        <p>Graph algorithms play a vital role in solving problems related to networks and connections, particularly through spanning trees and shortest paths. A spanning tree connects all vertices in a graph without cycles, using the minimum number of edges, which is essential for optimizing network design and infrastructure. Algorithms like Kruskal's and Prim's help identify minimum spanning trees for efficient communication networks and road systems. Shortest path algorithms, such

        <p>...such as Dijkstra's and Bellman-Ford, are crucial for finding the most efficient routes between nodes in a graph, which is vital for applications like GPS navigation, logistics, and network routing. These algorithms enable the identification of optimal paths, considering factors like distance, time, and cost, thereby facilitating informed decision-making and resource allocation.</p>
        <h3>Minimum Spanning Tree</h3>
        <ul>
            <li><strong>Kruskal's Algorithm:</strong> Selects the smallest edge that does not create a cycle.</li>
            <li><strong>Prim's Algorithm:</strong> Grows the tree from a starting node, adding the smallest edge that connects a new node.</li>
        </ul>
        <h3>Shortest Path</h3>
        <ul>
            <li><strong>Dijkstra's Algorithm:</strong> Finds the shortest path from a source node to all other nodes in a weighted graph.</li>
            <li><strong>Bellman-Ford Algorithm:</strong> Computes the shortest path from a source node to all other nodes, handling negative weight edges.</li>
        </ul>
    </section>
    <section id="dynamic-programming">
        <h2>Dynamic Programming: Breaking Down Complex Problems</h2>
        <img src="(link unavailable)" alt="Dynamic Programming" style="width: 100%; height: 500px; object-fit: cover;">
        <p>Dynamic programming is a method for solving complex problems by breaking them down into smaller subproblems, solving each only once, and storing the solutions to subproblems to avoid redundant computation. This approach is particularly useful for problems that have overlapping subproblems or that can be decomposed into smaller subproblems. By using dynamic programming, we can reduce the computational time and resources required to solve complex problems, making it an essential tool for solving many real-world problems.</p>
        <h3>Key Elements of Dynamic Programming</h3>
        <ul>
            <li><strong>Breaking down the problem:</strong> Divide the problem into smaller subproblems.</li>
            <li><strong>Overlapping subproblems:</strong> Subproblems may have some overlap, meaning that some subproblems may be identical or have similar solutions.</li>
            <li><strong>Optimal substructure:</strong> The problem can be broken down into smaller subproblems, and the optimal solution to the larger problem can be constructed from the optimal solutions of the subproblems.</li>
            <li><strong>Memoization:</strong> Store the solutions to subproblems to avoid redundant computation.</li>
        </ul>
        <h3>Examples of Dynamic Programming</h3>
        <ul>
            <li><strong>Fibonacci series:</strong> Calculate the nth Fibonacci number using dynamic programming.</li>
            <li><strong>Longest common subsequence:</strong> Find the longest common subsequence between two strings using dynamic programming.</li>
            <li><strong>Shortest path problems:</strong> Solve shortest path problems using dynamic programming.</li>
        </ul>
    </section>
    <section id="greedy-algorithms">
        <h2>Greedy Algorithms: Making Locally Optimal Choices</h2>
        <img src="(link unavailable)" alt="Greedy Algorithms" style="width: 100%; height: 500px; object-fit: cover;">
        <p>Greedy algorithms make locally optimal choices at each step with the hope of finding a global optimum solution. They are often used for optimization problems, where the goal is to minimize or maximize a particular objective function. Greedy algorithms can be used to solve a wide range of problems, including scheduling, resource allocation, and network optimization. However, they do not always produce the optimal solution and may get stuck in local optima.</p>
        <h3>Key Elements of Greedy Algorithms</h3>
        <ul>
            <li><strong>Greedy choice:</strong> Make a locally optimal choice at each step.</li>
            <li><strong>Hope for global optimum:</strong> Hope that the locally optimal choices lead to a global optimum solution.</li>
            <li><strong>No backtracking:</strong> Greedy algorithms do not backtrack or reconsider previous choices.</li>
        </ul>
        <h3>Examples of Greedy Algorithms</h3>
        <ul>
            <li><strong>Huffman coding:</strong> Construct a Huffman tree using a greedy algorithm.</li>
            <li><strong>Activity selection problem:</strong> Select the maximum number of activities that can be performed by a single person using a greedy algorithm.</li>
            <li><strong>Fractional knapsack problem:</strong> Solve the fractional knapsack problem using a greedy algorithm.</li>
        </ul>
    </section>
</body>
</html>

